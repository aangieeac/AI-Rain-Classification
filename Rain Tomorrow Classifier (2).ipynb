{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hYw8q9uBTCk"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NaJvIAQKUH6"
      },
      "source": [
        "Import Statements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7Pt8rzWBpf9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay,confusion_matrix\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.utils import resample\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJQA9_yXn7CM"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvT1vomLBtPX"
      },
      "outputs": [],
      "source": [
        "cd /content/drive/MyDrive/Colab Notebooks/Data Analytics python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Sk0CegXKWkL"
      },
      "source": [
        "Load Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuKizmfVB1Wr"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('Assignment3-WeatherData.csv')\n",
        "\n",
        "unknown_data_cleaned = pd.read_csv('Assignment3-UnknownData.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71xsLVgWoxmM"
      },
      "source": [
        "Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyfIj3NDZAx8"
      },
      "outputs": [],
      "source": [
        "#Checking the data before\n",
        "print(\"Number of Rows Initially: \" + str(data.shape[0]))\n",
        "print(\"Number of Rows with Null Values: \" + str(sum(data.isnull().values.ravel())))\n",
        "print(\"Null values in each column: \" + str(data.isnull().mean() * 100))\n",
        "\n",
        "null_count = data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RA5WAoIFk1_"
      },
      "outputs": [],
      "source": [
        "# Visualising null values using a bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "null_count.plot(kind='bar')\n",
        "plt.title('Number of Null Values per Column')\n",
        "plt.xlabel('Columns')\n",
        "plt.ylabel('Count of Null Values')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4V3zhs2HKkEx"
      },
      "source": [
        "Cleaning Data (Removing Nulls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eL4H5HZnCB9z"
      },
      "outputs": [],
      "source": [
        "numerical_columns = [\n",
        "    'MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine',\n",
        "    'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm',\n",
        "    'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm',\n",
        "    'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm'\n",
        "]\n",
        "\n",
        "categorical_columns = ['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday']\n",
        "\n",
        "\n",
        "#Copy of original dataframe for the cleaned version\n",
        "data_cleaned = data.copy()\n",
        "\n",
        "# Fill in numerical columns with the median of each column\n",
        "for col in numerical_columns:\n",
        "    data_cleaned[col] = data_cleaned[col].fillna(data_cleaned[col].median())\n",
        "\n",
        "# Fill in categorical columns with the mode of each column\n",
        "for col in categorical_columns:\n",
        "    data_cleaned[col] = data_cleaned[col].fillna(data_cleaned[col].mode()[0])\n",
        "\n",
        "print(data_cleaned.isnull().sum())\n",
        "\n",
        "for col in numerical_columns:\n",
        "    unknown_data_cleaned[col] = unknown_data_cleaned[col].fillna(unknown_data_cleaned[col].median())\n",
        "\n",
        "for col in categorical_columns:\n",
        "    unknown_data_cleaned[col] = unknown_data_cleaned[col].fillna(unknown_data_cleaned[col].mode()[0])\n",
        "\n",
        "print(unknown_data_cleaned.isnull().sum())\n",
        "\n",
        "unknown_data_cleaned.drop('row ID', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AzNSTqwbW0G"
      },
      "outputs": [],
      "source": [
        "data_cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PieCQQ2jIk2o"
      },
      "outputs": [],
      "source": [
        "#Scaling numerical features\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "for col in numerical_columns:\n",
        "  data_cleaned[col] = scaler.fit_transform(data_cleaned[[col]])\n",
        "\n",
        "for col in numerical_columns:\n",
        "  unknown_data_cleaned[col] = scaler.fit_transform(unknown_data_cleaned[[col]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4l35o5CqNwcK"
      },
      "outputs": [],
      "source": [
        "# Initialize encoders\n",
        "le_rain_today = LabelEncoder()\n",
        "encoder_gust_dir = OneHotEncoder(sparse_output=False)\n",
        "encoder_wind_9am = OneHotEncoder(sparse_output=False)\n",
        "encoder_wind_3pm = OneHotEncoder(sparse_output=False)\n",
        "le_location = LabelEncoder()\n",
        "\n",
        "# Encode RainToday column in data_cleaned\n",
        "data_cleaned['RainToday'] = le_rain_today.fit_transform(data_cleaned['RainToday'])\n",
        "\n",
        "# Frequency encoding for Location\n",
        "data_cleaned['Location'] = le_location.fit_transform(data_cleaned['Location'])\n",
        "\n",
        "# One-hot encode WindGustDir, WindDir9am, WindDir3pm in data_cleaned\n",
        "wind_gust_df = pd.DataFrame(encoder_gust_dir.fit_transform(data_cleaned[['WindGustDir']]),\n",
        "                            columns=encoder_gust_dir.get_feature_names_out(['WindGustDir']))\n",
        "data_cleaned = pd.concat([data_cleaned, wind_gust_df], axis=1)\n",
        "data_cleaned.drop('WindGustDir', axis=1, inplace=True)\n",
        "\n",
        "wind_9am_df = pd.DataFrame(encoder_wind_9am.fit_transform(data_cleaned[['WindDir9am']]),\n",
        "                           columns=encoder_wind_9am.get_feature_names_out(['WindDir9am']))\n",
        "data_cleaned = pd.concat([data_cleaned, wind_9am_df], axis=1)\n",
        "data_cleaned.drop('WindDir9am', axis=1, inplace=True)\n",
        "\n",
        "wind_3pm_df = pd.DataFrame(encoder_wind_3pm.fit_transform(data_cleaned[['WindDir3pm']]),\n",
        "                           columns=encoder_wind_3pm.get_feature_names_out(['WindDir3pm']))\n",
        "data_cleaned = pd.concat([data_cleaned, wind_3pm_df], axis=1)\n",
        "data_cleaned.drop('WindDir3pm', axis=1, inplace=True)\n",
        "\n",
        "# Apply the same transformations to unknown_data_cleaned\n",
        "unknown_data_cleaned['Location'] = le_location.transform(unknown_data_cleaned['Location'])\n",
        "unknown_data_cleaned['RainToday'] = le_rain_today.transform(unknown_data_cleaned['RainToday'])\n",
        "\n",
        "# One-hot encode WindGustDir, WindDir9am, WindDir3pm in unknown_data_cleaned\n",
        "wind_gust_unknown_df = pd.DataFrame(encoder_gust_dir.transform(unknown_data_cleaned[['WindGustDir']]),\n",
        "                                    columns=encoder_gust_dir.get_feature_names_out(['WindGustDir']))\n",
        "unknown_data_cleaned = pd.concat([unknown_data_cleaned, wind_gust_unknown_df], axis=1)\n",
        "unknown_data_cleaned.drop('WindGustDir', axis=1, inplace=True)\n",
        "\n",
        "wind_9am_unknown_df = pd.DataFrame(encoder_wind_9am.transform(unknown_data_cleaned[['WindDir9am']]),\n",
        "                                   columns=encoder_wind_9am.get_feature_names_out(['WindDir9am']))\n",
        "unknown_data_cleaned = pd.concat([unknown_data_cleaned, wind_9am_unknown_df], axis=1)\n",
        "unknown_data_cleaned.drop('WindDir9am', axis=1, inplace=True)\n",
        "\n",
        "wind_3pm_unknown_df = pd.DataFrame(encoder_wind_3pm.transform(unknown_data_cleaned[['WindDir3pm']]),\n",
        "                                   columns=encoder_wind_3pm.get_feature_names_out(['WindDir3pm']))\n",
        "unknown_data_cleaned = pd.concat([unknown_data_cleaned, wind_3pm_unknown_df], axis=1)\n",
        "unknown_data_cleaned.drop('WindDir3pm', axis=1, inplace=True)\n",
        "\n",
        "# Check for nulls and confirm encoding\n",
        "print(data_cleaned.isnull().sum())\n",
        "print(unknown_data_cleaned.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSzcmRg2R8Ac"
      },
      "outputs": [],
      "source": [
        "X = data_cleaned.drop('RainTomorrow', axis=1)\n",
        "y = data_cleaned['RainTomorrow']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6MNnu1wR2eC"
      },
      "outputs": [],
      "source": [
        "#Before balancing\n",
        "\n",
        "class_counts = y.value_counts()\n",
        "print(class_counts)\n",
        "\n",
        "# Plot the counts\n",
        "plt.figure(figsize=(8, 6))\n",
        "class_counts.plot(kind='pie')\n",
        "plt.title('Counts of 0s and 1s in RainTomorrow')\n",
        "plt.xlabel('RainTomorrow (0 = No, 1 = Yes)')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-n1QCm4syoN"
      },
      "outputs": [],
      "source": [
        "# Method 1\n",
        "X = data_cleaned.drop('RainTomorrow', axis=1)\n",
        "y = data_cleaned['RainTomorrow']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Separate majority and minority classes\n",
        "majority_class = X_train[y_train == 0]\n",
        "minority_class = X_train[y_train == 1]\n",
        "\n",
        "# Downsample the majority class to 75% of its original size\n",
        "majority_downsampled = resample(\n",
        "    majority_class,\n",
        "    replace=False,\n",
        "    n_samples=int(len(minority_class) * 1.5),\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Combine the downsampled majority class with the minority class\n",
        "X_combined = pd.concat([majority_downsampled, minority_class])\n",
        "y_combined = pd.concat([y_train[y_train == 0].iloc[:len(majority_downsampled)], y_train[y_train == 1]])\n",
        "\n",
        "# Use SMOTE to balance the classes equally\n",
        "smote = SMOTE(sampling_strategy= 'auto', random_state=42)\n",
        "X_balanced, Y_balanced = smote.fit_resample(X_combined, y_combined)\n",
        "\n",
        "\n",
        "# #Method 2\n",
        "# minority_class = data_cleaned[data_cleaned['RainTomorrow'] == 1]\n",
        "# majority_class = data_cleaned[data_cleaned['RainTomorrow'] == 0]\n",
        "\n",
        "# # Downsample the majority class\n",
        "# majority_downsampled = resample(majority_class, replace=False, n_samples=len(minority_class), random_state=42)\n",
        "\n",
        "# # Combine the downsampled majority class with the minority class\n",
        "# balanced_data = pd.concat([minority_class, majority_downsampled])\n",
        "\n",
        "# X_balanced = balanced_data.drop('RainTomorrow', axis = 1)\n",
        "# Y_balanced = balanced_data['RainTomorrow']\n",
        "\n",
        "# # Check class distribution\n",
        "# counter = Counter(Y_balanced)\n",
        "# print(\"Class distribution:\", counter)\n",
        "\n",
        "# #Method 3\n",
        "# from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# X = data_cleaned.drop('RainTomorrow', axis=1)\n",
        "# y = data_cleaned['RainTomorrow']\n",
        "\n",
        "# # Split data into train and test sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Check initial class distribution\n",
        "# counter = Counter(y_train)\n",
        "# print(\"Original training set class distribution:\", counter)\n",
        "\n",
        "# # Step 1: Downsample majority class to 1.25 times the minority class size\n",
        "# majority_class_size = int(counter[1] * 2)\n",
        "# under_sampler = RandomUnderSampler(sampling_strategy={0: majority_class_size, 1: counter[1]}, random_state=42)\n",
        "# X_train_resampled, y_train_resampled = under_sampler.fit_resample(X_train, y_train)\n",
        "\n",
        "# # Check distribution after downsampling\n",
        "# print(\"Class distribution after downsampling:\", Counter(y_train_resampled))\n",
        "\n",
        "# # Step 2: Apply SMOTE to further balance classes\n",
        "# smote = SMOTE(sampling_strategy=0.51, random_state=42)  # Creates synthetic samples until classes are 95% balanced\n",
        "# X_balanced, y_balanced = smote.fit_resample(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# # Check final class distribution\n",
        "# print(\"Final balanced class distribution:\", Counter(y_balanced))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqXLwew1SAwM"
      },
      "outputs": [],
      "source": [
        "#After balancing\n",
        "balanced_counts = Y_balanced.value_counts()\n",
        "print(balanced_counts)\n",
        "\n",
        "# Plot the counts\n",
        "plt.figure(figsize=(8, 6))\n",
        "balanced_counts.plot(kind='pie')\n",
        "plt.title('Counts of 0s and 1s in RainTomorrow')\n",
        "plt.xlabel('RainTomorrow (0 = No, 1 = Yes)')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmB0Dr7qY-Ba"
      },
      "outputs": [],
      "source": [
        "dt = DecisionTreeClassifier(criterion='gini',\n",
        "    max_depth=5,\n",
        "    min_samples_split = 2,\n",
        "    min_samples_leaf=2,\n",
        "    random_state=42\n",
        ")\n",
        "dt.fit(X_balanced, Y_balanced)\n",
        "\n",
        "y_preds_dt = dt.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_preds_dt))\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (5,4), dpi = 100)\n",
        "cm = confusion_matrix(y_test, y_preds_dt)\n",
        "cmp = ConfusionMatrixDisplay(cm, display_labels = [\"Negative\", \"Positive\"])\n",
        "cmp.plot(ax=ax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFFXVQ7rkc5l"
      },
      "outputs": [],
      "source": [
        "# Initialize KNN model\n",
        "knn = KNeighborsClassifier(\n",
        "    n_neighbors=25,\n",
        "    weights='distance',\n",
        "    metric='manhattan',\n",
        "    p=1\n",
        ")\n",
        "\n",
        "knn.fit(X_balanced, Y_balanced)\n",
        "\n",
        "y_pred_knn = knn.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred_knn))\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (5,4), dpi = 100)\n",
        "cm = confusion_matrix(y_test, y_pred_knn)\n",
        "cmp = ConfusionMatrixDisplay(cm, display_labels = [\"Negative\", \"Positive\"])\n",
        "cmp.plot(ax=ax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vt1bUK08lhMQ"
      },
      "outputs": [],
      "source": [
        "rff = RandomForestClassifier(\n",
        "    n_estimators = 400,\n",
        "    criterion='entropy',\n",
        "    random_state = 42\n",
        ")\n",
        "\n",
        "rff.fit(X_balanced, Y_balanced)\n",
        "y_preds_rff = rff.predict(X_test)\n",
        "print(classification_report(y_test, y_preds_rff))\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (5,4), dpi = 100)\n",
        "cm = confusion_matrix(y_test, y_preds_rff)\n",
        "cmp = ConfusionMatrixDisplay(cm, display_labels = [\"Negative\", \"Positive\"])\n",
        "cmp.plot(ax=ax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Fal-qfPmaFE"
      },
      "outputs": [],
      "source": [
        "svc = SVC(C=10, kernel='rbf', gamma='scale', random_state=42)\n",
        "\n",
        "svc.fit(X_balanced, Y_balanced)\n",
        "y_preds_svc = svc.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_preds_svc))\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (5,4), dpi = 100)\n",
        "cm = confusion_matrix(y_test, y_preds_svc)\n",
        "cmp = ConfusionMatrixDisplay(cm, display_labels = [\"Negative\", \"Positive\"])\n",
        "cmp.plot(ax=ax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1uYs-TWnz9W"
      },
      "outputs": [],
      "source": [
        "# Initialize the Perceptron\n",
        "pt = MLPClassifier(hidden_layer_sizes=(200,100,50,25),\n",
        "                    activation='relu',\n",
        "                    solver=\"sgd\",\n",
        "                    alpha=0.00001,\n",
        "                    batch_size='auto',\n",
        "                    learning_rate='adaptive',\n",
        "                    learning_rate_init=0.0001,\n",
        "                    max_iter=10000,\n",
        "                    shuffle=True,\n",
        "                    random_state=42,\n",
        "                    early_stopping=True)\n",
        "\n",
        "pt.fit(X_balanced, Y_balanced)\n",
        "\n",
        "y_pred_pt = pt.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred_pt))\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (5,4), dpi = 100)\n",
        "cm = confusion_matrix(y_test, y_pred_pt)\n",
        "cmp = ConfusionMatrixDisplay(cm, display_labels = [\"Negative\", \"Positive\"])\n",
        "cmp.plot(ax=ax)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
